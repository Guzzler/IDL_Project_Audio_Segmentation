{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB1i1GWBHMtA"
   },
   "source": [
    "# Initial Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJjOWmynluge"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "from subprocess import Popen, PIPE\n",
    "from keras import regularizers\n",
    "from os.path import dirname\n",
    "import os\n",
    "import soundfile as sf\n",
    "!sudo apt-get install sox\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "!git clone https://github.com/DemisEom/SpecAugment.git\n",
    "!pip install /content/SpecAugment/ --quiet\n",
    "!pip install tensorflow-addons --quiet\n",
    "!pip install sed_eval --quiet\n",
    "import keras\n",
    "from SpecAugment import spec_augment_tensorflow\n",
    "import sed_eval\n",
    "import dcase_util\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oplts-YdlJeV"
   },
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "NRQ9DlXiWdYi",
    "outputId": "3d0e523d-0bd9-485c-ec86-1f8b2f3a209c"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "output1 = \"/content/new-dataset/test_data1.zip\"\n",
    "gdown.download(id='17PvyXLEkpIgBLxRkuTIk25MGL3uM3kS1', output=output1, quiet=False)\n",
    "\n",
    "output2= \"/content/new-dataset/test_data2.zip\"\n",
    "gdown.download(id='14abMPBH3EVmcU-3jPD4jWEpa4pQ38OY9', output=output2, quiet=False)\n",
    "\n",
    "output3 = \"/content/new-dataset/test_data3.zip\"\n",
    "gdown.download(id='1TU4CoJuFy40-zJopo3R4U-YgZFKFrtxB', output=output3, quiet=False)\n",
    "\n",
    "output4 = \"/content/new-dataset/test_data4.zip\"\n",
    "gdown.download(id='1E5595RX2NwpuckXvl2o1V_dm9Ja58arF', output=output4, quiet=False)\n",
    "\n",
    "output5 = \"/content/new-dataset/test_data5.zip\"\n",
    "gdown.download(id='16VJhkCV2-ILcHxiF8a2ygEiHxXyF9RtM', output=output5, quiet=False)\n",
    "\n",
    "output6 = \"/content/new-dataset/test_data6.zip\"\n",
    "gdown.download(id='1kZyXyZVTHnSTg-gdrlf91V5ioYMr50Mp', output=output6, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_p7z3-QHotX"
   },
   "source": [
    "# Unzip all The data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4af7CjopKs0E"
   },
   "outputs": [],
   "source": [
    "def unzip_data(download_path, extract_path):\n",
    "\n",
    "  # create glob\n",
    "  final_glob = glob.glob(f\"{download_path}*.zip\")\n",
    "\n",
    "  for zip_name in final_glob:\n",
    "    with ZipFile(zip_name, 'r') as zip:\n",
    "      zip.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLu4m-Q1Hnd0"
   },
   "outputs": [],
   "source": [
    "download_test_path = '/content/new-dataset/'\n",
    "extract_test_path = '/content/extracted-data'\n",
    "\n",
    "unzip_data(download_test_path, extract_test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkRc04QOHwA3"
   },
   "source": [
    "# Extract Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v36BZwiSDZg1"
   },
   "outputs": [],
   "source": [
    "def convert_annotations_to_events(filename): #read_annotations\n",
    "    events = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            row.append(row[0])\n",
    "            row.pop(0)\n",
    "            row[1] = str((float(row[1])/1000))\n",
    "            row[0] = str((float(row[0])/1000))\n",
    "            events.append(row)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6CZDBeoDaPy",
    "outputId": "5ff7ab13-c394-499d-becd-a94bda7b2144"
   },
   "outputs": [],
   "source": [
    "events = convert_annotations_to_events(\"/content/extracted-data/outputs/0.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJIKX8zVlfDX"
   },
   "source": [
    "## Preprocess Audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcjhqP6-Mbuj"
   },
   "outputs": [],
   "source": [
    "audio_files = glob.glob(\"/content/extracted-data/outputs/*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwG5VpfZUeeV"
   },
   "outputs": [],
   "source": [
    "os.makedirs(dirname(audio_files[0]).replace(\"outputs\", \"outputs-mono\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Flli3LPbTAEK"
   },
   "outputs": [],
   "source": [
    "for sound in audio_files:\n",
    "  temp_file = sound.replace(\"outputs\", \"outputs-mono\")\n",
    "  command = command = \"sox \" + sound + \" \" + temp_file + \" channels 1\"\n",
    "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "  output, err = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dY_Ri78WFR_"
   },
   "outputs": [],
   "source": [
    "audio_files_mono = glob.glob(\"/content/extracted-data/outputs-mono/*.wav\")\n",
    "random.shuffle(audio_files_mono)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VElzHF_ss3T1"
   },
   "source": [
    "# Split into folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gk-V6K0SCNt4",
    "outputId": "c8d9a784-7414-407f-f820-5e59764f30f1"
   },
   "outputs": [],
   "source": [
    "fold1_train_files = []\n",
    "fold1_val_files = []\n",
    "fold1_test_files = []\n",
    "i = 0\n",
    "# Do the 70,20,10 train, val, test split\n",
    "for f in audio_files_mono:\n",
    "  if i < (70* len(audio_files_mono)// 100):\n",
    "    fold1_train_files.append(f)\n",
    "  elif i < (70* len(audio_files_mono)// 100):\n",
    "    fold1_val_files.append(f)\n",
    "  else:\n",
    "    fold1_test_files.append(f)\n",
    "  i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRRabjwJl9bs"
   },
   "source": [
    "## Dataset Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUElUf__xD3M"
   },
   "outputs": [],
   "source": [
    "def construct_examples(audio_path, win_len = 2.56, hop_len = 1.0, sr = 44100.0):\n",
    "  # here win_len is the window_length and hop_len is the hop_length between the examples.\n",
    "  # sr is the sampling rate\n",
    "\n",
    "  window_length_t = win_len\n",
    "  hop_length_t = hop_len\n",
    "\n",
    "  window_length = int(sr*window_length_t)\n",
    "  hop_length = int(sr*hop_length_t)\n",
    "\n",
    "  audio, sr = sf.read(audio_path)\n",
    "\n",
    "  # handle padding\n",
    "  if audio.shape[0] < window_length:\n",
    "    audio_padded = np.zeros((window_length, ))\n",
    "    audio_padded[0:audio.shape[0]] = audio \n",
    "\n",
    "  else:\n",
    "    no_of_hops = math.ceil((audio.shape[0] - window_length) / hop_length)\n",
    "    audio_padded = np.zeros((int(window_length + hop_length*no_of_hops), ))\n",
    "    audio_padded[0:audio.shape[0]] = audio  \n",
    "\n",
    "  audio_example = [audio_padded[i - window_length : i] for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "  win_ranges = [((i - window_length)/sr, i/sr) for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "\n",
    "  return audio_example, win_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LScHoJOTkc6"
   },
   "outputs": [],
   "source": [
    "def construct_labels(annotation_path, win_start, win_end, win_len):\n",
    "  # takes the annotation_path, window_start, window_end and window_length\n",
    "  events = convert_annotations_to_events(annotation_path)\n",
    "\n",
    "  annotation_vals = [[float(e[0]), float(e[1]), e[2]] for e in events]\n",
    "\n",
    "  curr_annotation = []\n",
    "\n",
    "  for annotation in annotation_vals:\n",
    "    if annotation[1] > win_start and annotation[0] <= win_end: \n",
    "      curr_start = max(annotation[0] - win_start, 0.0)\n",
    "      curr_end = min(annotation[1] - win_start, win_len)\n",
    "      curr_annotation.append([curr_start, curr_end, annotation[2]])    \n",
    "\n",
    "  # get current class set from annotations\n",
    "  class_set = set([c[2] for c in curr_annotation])\n",
    "  class_wise_events = {}\n",
    "\n",
    "  for c in list(class_set):\n",
    "    class_wise_events[c] = []\n",
    "\n",
    "\n",
    "  for c in curr_annotation:\n",
    "    class_wise_events[c[2]].append(c)\n",
    "    \n",
    "  max_event_silence = 0.0\n",
    "  all_events = []\n",
    "\n",
    "  for k in list(class_wise_events.keys()):\n",
    "    curr_events = class_wise_events[k]\n",
    "    count = 0\n",
    "\n",
    "    while count < len(curr_events) - 1:\n",
    "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
    "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
    "        del curr_events[count + 1]\n",
    "      else:\n",
    "        count += 1\n",
    "\n",
    "    all_events += curr_events\n",
    "\n",
    "  for i in range(len(all_events)):\n",
    "    # round all the values so that they are not arbitarily long\n",
    "    all_events[i][0] = round(all_events[i][0], 3)\n",
    "    all_events[i][1] = round(all_events[i][1], 3)\n",
    "\n",
    "  all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "  return all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b32KeTkwsLEk"
   },
   "outputs": [],
   "source": [
    "def get_universal_labels(events, class_dict, ex_length = 10.0, no_of_div = 32):\n",
    "  # returns all labels from events\n",
    "  win_length = ex_length/no_of_div\n",
    "  labels = np.zeros((no_of_div, len(class_dict.keys()) * 3))\n",
    "  \n",
    "  for e in events:\n",
    "\n",
    "    start_time = float(e[0])\n",
    "    stop_time = float(e[1])\n",
    "\n",
    "    # to prevent edge case issues\n",
    "    if (float(e[0]) == 2.56):\n",
    "      start_time = float(e[0] - 0.00001)\n",
    "    if (float(e[1] == 2.56)):\n",
    "      stop_time = float(e[1]- 0.000001)\n",
    "     \n",
    "\n",
    "    start_bin = int(start_time // win_length)\n",
    "    stop_bin = int(stop_time // win_length)\n",
    "\n",
    "    start_time_2 = start_time - start_bin * win_length\n",
    "    stop_time_2 = stop_time - stop_bin * win_length\n",
    "\n",
    "    n_bins = stop_bin - start_bin\n",
    "\n",
    "    if n_bins == 0:\n",
    "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, stop_time_2]    \n",
    "\n",
    "    elif n_bins == 1:\n",
    "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, win_length]\n",
    "\n",
    "      if stop_time_2 > 0.0:\n",
    "        labels[stop_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, stop_time_2]\n",
    "\n",
    "    elif n_bins > 1:\n",
    "      labels[start_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, start_time_2, win_length]\n",
    "\n",
    "      for i in range(1, n_bins):\n",
    "        labels[start_bin + i, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, win_length]\n",
    "\n",
    "      if stop_time_2 > 0.0:\n",
    "        labels[stop_bin, class_dict[e[2]] * 3:class_dict[e[2]] * 3 + 3] = [1, 0.0, stop_time_2]\n",
    "\n",
    "  # divide all time values by window_length\n",
    "  for labelIndex in range(len(labels)):\n",
    "    for valIndex in range(len(labels[labelIndex])):\n",
    "      if valIndex % 3 != 0:\n",
    "        labels[labelIndex][valIndex] /= win_length\n",
    "\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh8YoqFVNhVf"
   },
   "outputs": [],
   "source": [
    "CLASS_ENCODING = {\"car\": 0, \"aircraft\": 1, \"crowds\":2, \"footsteps\":3, \"clocks\":4, \"rainforest\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWXK4awdQmx8"
   },
   "source": [
    "# Construct Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mLnGwjXvm_P"
   },
   "outputs": [],
   "source": [
    "def construct_data_set(fold_files, path):\n",
    "  shutil.rmtree(path, ignore_errors=True)\n",
    "  os.mkdir(path)\n",
    "\n",
    "  window_length = 2.56\n",
    "  hop_length = 1.0\n",
    "  a_examples_train = []\n",
    "  a_labels_train = []\n",
    "\n",
    "\n",
    "  for i, audio in enumerate(fold_files):\n",
    "    # get \n",
    "    a, window_ranges = construct_examples(audio,win_len=window_length, hop_len=hop_length)\n",
    "    a_examples_train += a\n",
    "\n",
    "    for w in window_ranges:\n",
    "      labels_t = construct_labels(audio.replace(\".wav\", \".txt\").replace('outputs-mono', 'outputs'), w[0], w[1], win_len=window_length)\n",
    "      ll = get_universal_labels(labels_t, CLASS_ENCODING, ex_length=window_length, no_of_div = 9)\n",
    "      a_labels_train.append(ll)\n",
    "  return a_examples_train, a_labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUBSH4RDQ6_G"
   },
   "outputs": [],
   "source": [
    "examples_train, labels_train = construct_data_set(fold1_train_files, '/content/train-data')\n",
    "examples_val, labels_val = construct_data_set(fold1_val_files, '/content/val-data')\n",
    "examples_test, labels_test = construct_data_set(fold1_test_files, '/content/test-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ru11Of8RMzq"
   },
   "source": [
    "# Extract MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz6t6EslAhsJ"
   },
   "outputs": [],
   "source": [
    "def get_log_melspectrogram(audio, sr = 44100, hop_length = 441, win_length = 1764, n_fft = 2048, n_mels = 40, fmin = 0, fmax = 22050):\n",
    "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
    "    audio_2 = librosa.util.normalize(audio)\n",
    "    bands = librosa.feature.melspectrogram(\n",
    "        y=audio_2, sr=sr, hop_length=hop_length, win_length = win_length, n_fft=n_fft, n_mels=n_mels)\n",
    "    return librosa.core.power_to_db(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mSPzyS1-Q9-"
   },
   "outputs": [],
   "source": [
    "# save melspectrograms for entire set\n",
    "def save_example_mel(example_set, save_path):\n",
    "  for i, audio in enumerate(example_set):\n",
    "    M = get_log_melspectrogram(audio).T\n",
    "    # print(M.shape)\n",
    "    np.save(save_path + str(i) + \".npy\", M)\n",
    "\n",
    "# save labels in numpy format\n",
    "def save_labels_np(label_set, save_path):\n",
    "  for i, audio in enumerate(label_set):\n",
    "    np.save(save_path + str(i) + \".npy\", audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R7pthk1-jjJ"
   },
   "outputs": [],
   "source": [
    "# save dataset for all train set\n",
    "train_path_ex = '/content/train-data/ex-'\n",
    "train_path_labels = '/content/train-data/label-'\n",
    "val_path_ex = '/content/val-data/ex-'\n",
    "val_path_labels = '/content/val-data/label-'\n",
    "test_path_ex = '/content/test-data/ex-'\n",
    "test_path_labels = '/content/test-data/label-'\n",
    "\n",
    "\n",
    "\n",
    "save_example_mel(examples_train, train_path_ex)\n",
    "save_labels_np(labels_train, train_path_labels)\n",
    "save_example_mel(examples_val, val_path_ex)\n",
    "save_labels_np(labels_val, val_path_labels)\n",
    "save_example_mel(examples_test, test_path_ex)\n",
    "save_labels_np(labels_test, test_path_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvOuMznFRd09"
   },
   "source": [
    "# Sort and Partition Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvMbxJLj9x5n"
   },
   "outputs": [],
   "source": [
    "def intOrVal(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "    \n",
    "def alphanum_key(init_string):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [intOrVal(c) for c in re.split('([0-9]+)', init_string)]\n",
    "\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjtncXqB91qu"
   },
   "outputs": [],
   "source": [
    "def get_sorted_data(regex_path):\n",
    "  data = glob.glob(regex_path) \n",
    "  sort_nicely(data)\n",
    "  return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load the individual numpy arrays into partition\n",
    "\"\"\"\n",
    "train_data_examples_regex_path = \"/content/train-data/ex-*.npy\"\n",
    "train_data_labels_regex_path = \"/content/train-data/label-*.npy\"\n",
    "val_data_examples_regex_path = \"/content/val-data/ex-*.npy\"\n",
    "val_data_labels_regex_path = \"/content/val-data/label-*.npy\"\n",
    "test_data_examples_regex_path = \"/content/test-data/ex-*.npy\"\n",
    "test_data_labels_regex_path = \"/content/test-data/label-*.npy\"\n",
    "\n",
    "train_data = get_sorted_data(train_data_examples_regex_path)\n",
    "train_labels = get_sorted_data(train_data_labels_regex_path)\n",
    "\n",
    "val_data = get_sorted_data(val_data_examples_regex_path)\n",
    "val_labels = get_sorted_data(val_data_labels_regex_path)\n",
    "\n",
    "test_data = get_sorted_data(test_data_examples_regex_path)\n",
    "test_labels = get_sorted_data(test_data_labels_regex_path)\n",
    "\n",
    "training_examples = [(train_data[i], train_labels[i]) for i in range(len(train_data))]\n",
    "validation_examples = [(val_data[i], val_labels[i]) for i in range(len(val_data))]\n",
    "test_examples = [(test_data[i], test_labels[i]) for i in range(len(test_data))]\n",
    "\n",
    "# shuffle all examples\n",
    "random.seed(7)\n",
    "random.shuffle(training_examples)\n",
    "random.shuffle(validation_examples)\n",
    "random.shuffle(test_examples)\n",
    "partition = {}\n",
    "partition['train'] = training_examples\n",
    "partition['validation'] = validation_examples\n",
    "partition['test'] = test_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJfMrLr0Tuxg"
   },
   "source": [
    "# Setup Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I5anQDD4b9b"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_examples, batch_size=128, shuffle=True):\n",
    "        # dim\n",
    "        # self.dim = (1,)\n",
    "        self.batch_size = batch_size\n",
    "        self.list_examples = list_examples\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # initial shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_examples) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_examples[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y = self.generate_data(list_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "      self.indexes = np.arange(len(self.list_examples))\n",
    "\n",
    "      # shuffle indexes at end of epoch\n",
    "      if self.shuffle == True:\n",
    "          np.random.shuffle(self.indexes)\n",
    "\n",
    "    def generate_data(self, list_IDs_temp):\n",
    "        # 'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        X = np.empty([self.batch_size, 257, 40, 1], dtype=np.float64)\n",
    "        Y = np.empty([self.batch_size, 9, 18], dtype=np.float64)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "          # Store sample\n",
    "          # load npy array\n",
    "          np_x = np.load(ID[0])\n",
    "\n",
    "          X[i, :, :, 0] = np_x\n",
    "\n",
    "          # load class label\n",
    "          np_y = np.load(ID[1])\n",
    "          Y[i, :, :] = np_y\n",
    "\n",
    "        tau = X.shape[1]          \n",
    "        v = X.shape[2]\n",
    "\n",
    "        # frequency and time masking of X values\n",
    "        warped_frequency_spectrogram = spec_augment_tensorflow.frequency_masking(X, v=v,  frequency_masking_para=8, frequency_mask_num=1)\n",
    "        warped_frequency_time_spectrogram = spec_augment_tensorflow.time_masking(warped_frequency_spectrogram, tau=tau, time_masking_para=25, time_mask_num=2)\n",
    "\n",
    "        X = warped_frequency_time_spectrogram\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylXiTH1Q_0vY"
   },
   "outputs": [],
   "source": [
    "# Parametersa\n",
    "params = {'batch_size': 64, 'shuffle': True}\n",
    "\n",
    "training_generator = DataGenerator(partition['train'], **params)\n",
    "validation_generator = DataGenerator(partition['validation'], **params)\n",
    "test_generator = DataGenerator(partition['test'], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp6_rpxJ-5FA"
   },
   "source": [
    "# Define the YOHO network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAHsOD5v-6no"
   },
   "outputs": [],
   "source": [
    "def square_difference_loss(y_true, y_pred):\n",
    "  squared_difference = tf.square(y_true - y_pred)\n",
    "\n",
    "  ss_True = squared_difference[:, :, 0] * 0 + 1 \n",
    "  # ss_True is batchsize, window_len of 1s\n",
    "\n",
    "  # get every 3 value of y_true\n",
    "  ss_0 = y_true[:, :, 0]\n",
    "  ss_1 = y_true[:, :, 3]\n",
    "  ss_2 = y_true[:, :, 6]\n",
    "  ss_3 = y_true[:, :, 9]\n",
    "  ss_4 = y_true[:, :, 12]\n",
    "  ss_5 = y_true[:, :, 15]\n",
    "  # labels across all batch sizes\n",
    "\n",
    "  # stack values\n",
    "  stacked_ss = tf.stack((ss_True, ss_0, ss_0,\n",
    "                         ss_True, ss_1, ss_1,\n",
    "                         ss_True, ss_2, ss_2,\n",
    "                         ss_True, ss_3, ss_3,\n",
    "                         ss_True, ss_4, ss_4,\n",
    "                         ss_True, ss_5, ss_5), axis = 2)\n",
    "  \n",
    "  squared_difference =  tf.multiply(squared_difference, stacked_ss)\n",
    "\n",
    "  return tf.reduce_sum(squared_difference, axis=[-1, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6sEYWpa2zKd"
   },
   "outputs": [],
   "source": [
    "# Creates mel spctrograms for validation fold for training\n",
    "def create_val_melspectrograms():\n",
    "  win_length = 2.56\n",
    "  hop_size = 1.96\n",
    "  mss_ins = []\n",
    "  win_ranges_list = []\n",
    "\n",
    "\n",
    "  for ii, audio in enumerate(fold1_val_files): # why val?\n",
    "    a, win_ranges = construct_examples(audio, win_len=win_length,hop_len=hop_size)\n",
    "\n",
    "    mss_in = np.zeros((len(a), 257, 40))\n",
    "\n",
    "    preds = np.zeros((len(a), 9, 18))\n",
    "\n",
    "    for i in range(len(a)):\n",
    "      M = get_log_melspectrogram(a[i])\n",
    "      mss_in[i, :, :] = M.T\n",
    "    mss_ins.append(mss_in)\n",
    "    win_ranges_list.append(win_ranges)\n",
    "  return mss_ins,win_ranges_list\n",
    "\n",
    "\n",
    "BASE_MSS_INS, BASE_WIN_RANGE = create_val_melspectrograms()\n",
    "\n",
    "def mk_preds_YOHO_mel(model, ind, window_range_list=BASE_WIN_RANGE, mss_ins=BASE_MSS_INS, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
    "  preds = model.predict(mss_ins[ind])\n",
    "  events = []\n",
    "\n",
    "  for i in range(len(preds)):\n",
    "    p = preds[i, :, :]\n",
    "    events_curr = []\n",
    "    win_width = win_length / no_of_div\n",
    "    for predIdx in range(len(p)):\n",
    "      for classIdx in range(0, 6):\n",
    "        if p[predIdx][classIdx*3] >= 0.5:\n",
    "          start = win_width * predIdx + win_width * p[predIdx][classIdx*3+1] + window_range_list[ind][i][0]\n",
    "          end = p[predIdx][classIdx*3+2] * win_width + start\n",
    "          events_curr.append([start, end, rev_class_list[classIdx]])\n",
    "\n",
    "    events += events_curr\n",
    "\n",
    "\n",
    "  class_set = set([c[2] for c in events])\n",
    "  class_wise_events = {}\n",
    "\n",
    "  for c in list(class_set):\n",
    "    class_wise_events[c] = []\n",
    "\n",
    "\n",
    "  for c in events:\n",
    "    class_wise_events[c[2]].append(c)\n",
    "    \n",
    "  \n",
    "  all_events = []\n",
    "\n",
    "  for k in list(class_wise_events.keys()):\n",
    "    curr_events = class_wise_events[k]\n",
    "    count = 0\n",
    "\n",
    "    while count < len(curr_events) - 1:\n",
    "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
    "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
    "        del curr_events[count + 1]\n",
    "      else:\n",
    "        count += 1\n",
    "\n",
    "    all_events += curr_events\n",
    "\n",
    "  for i in range(len(all_events)):\n",
    "    all_events[i][0] = round(all_events[i][0], 3)\n",
    "    all_events[i][1] = round(all_events[i][1], 3)\n",
    "\n",
    "  all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "  return all_events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF3BfLy9Rt0Q",
    "outputId": "ab7d995c-91c2-4f82-fa70-ca29ffe1b634"
   },
   "outputs": [],
   "source": [
    "rev_class_list = list(CLASS_ENCODING.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xteY8W6XlaR_"
   },
   "outputs": [],
   "source": [
    "def frames_to_time(f, sr = 44100.0, hop_size = 441):\n",
    "  return f * hop_size / sr\n",
    "\n",
    "def preds_to_se(p, win_start, audio_clip_length = 2.56):\n",
    "  start_dicts = [-100, -100, -100, -100, -100, -100]\n",
    "  stop_dicts = [-100, -100, -100, -100, -100, -100]\n",
    "\n",
    "\n",
    "  start_speech = -100\n",
    "  start_music = -100\n",
    "  stop_speech = -100\n",
    "  stop_music = -100\n",
    "\n",
    "  audio_events = []\n",
    "\n",
    "  n_frames = p.shape[0]\n",
    "\n",
    "  for j in range(p.shape[1]):\n",
    "    if p[0, j] >= 0.5:\n",
    "      start_dicts[j] = 0\n",
    "\n",
    "  for j in range(p.shape[1]):\n",
    "    for i in range(n_frames - 1):\n",
    "      if p[i, j] < 0.5 and p[i+1, j] >= 0.5:\n",
    "        start_dicts[j] = i+1\n",
    "\n",
    "      elif p[i, j] >= 0.5 and p[i + 1, j] < 0.5:\n",
    "        stop_dicts[j] = i\n",
    "        start_time = frames_to_time(start_dicts[j])\n",
    "        stop_time = frames_to_time(stop_dicts[j])\n",
    "\n",
    "        audio_events.append([start_time+win_start, stop_time+win_start, rev_class_list[j]])\n",
    "        start_dicts[j] = -100\n",
    "        stop_dicts[j] = -100\n",
    "\n",
    "    if start_dicts[j] != -100:\n",
    "      start_time = frames_to_time(start_dicts[j])\n",
    "      stop_time = audio_clip_length\n",
    "      audio_events.append([start_time+win_start, stop_time+win_start, rev_class_list[j]])\n",
    "      start_dicts[j] = -100\n",
    "      stop_dicts[j] = -100\n",
    "\n",
    "  audio_events.sort(key = lambda x: x[0]) \n",
    "  return audio_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lhO-or3cxJm"
   },
   "outputs": [],
   "source": [
    "def extract_eval_labels_final(annotation_path):\n",
    "  events = convert_annotations_to_events(annotation_path)\n",
    "\n",
    "  ann = [[float(e[0]), float(e[1]), e[2]] for e in events]\n",
    "  \n",
    "  n_label = \"/content/final-eval/\" + os.path.basename(annotation_path)\n",
    "\n",
    "  with open(n_label, 'w') as fp:\n",
    "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2pIA6vCcxJm"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('/content/final-eval/', ignore_errors=True)\n",
    "os.mkdir(\"/content/final-eval/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTXX7tItcxJn"
   },
   "outputs": [],
   "source": [
    "for audio in fold1_val_files:\n",
    "  extract_eval_labels_final(audio.replace(\".wav\", \".txt\").replace(\"outputs-mono\", \"outputs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908,
     "referenced_widgets": [
      "d86553659a904b3aa3f8a16e1bc9309e",
      "2844f9b545184ae4857293bee5a31052",
      "c1d70626f56c4ef291d1322ee86e1a3c",
      "71f6ebf10eac41abb093e05fa07fbfdf",
      "a0766ee9e6024542b4a4fd76130ff0e7",
      "b3124f87e101407b8fa4fe77b93ca84a",
      "b1271aa0be754075b10daa109e3e5605",
      "a23928937eaf4a10b731d26b02d3880e"
     ]
    },
    "id": "tC2A9LCHVZHf",
    "outputId": "183c4fea-3917-4b15-f3ae-e509f7d3ba34"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"<yourwandb key>\")\n",
    "run = wandb.init(\n",
    "    name = \"YOHOArch - MelBandsChange\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    project = \"idl-project\", ### Project should be created in your wandb account \n",
    "    config = {\n",
    "        'lr': 1e-3,\n",
    "        'architecture': 'YOHO-MelBandsChange',\n",
    "        \n",
    "    } ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9RIHH_49zOE"
   },
   "outputs": [],
   "source": [
    "class KerasFinalCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    super(KerasFinalCallback, self).__init__()\n",
    "    self.best_f1 = 0.0\n",
    "    self.best_error = np.inf\n",
    "    \n",
    "  def on_train_begin(self, logs=None):\n",
    "    pass\n",
    "\n",
    "  def on_train_end(self, logs=None):\n",
    "    pass\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if epoch > 1:\n",
    "      for ii, audio in enumerate(fold1_val_files):\n",
    "        audio_file_path = audio\n",
    "        see = mk_preds_YOHO_mel(self.model, ii)\n",
    "        n_label = \"/content/eval-files-2/\" + os.path.basename(audio_file_path).replace(\".wav\" ,\"\") + \"-se-prediction.txt\"\n",
    "\n",
    "        with open(n_label, 'w') as fp:\n",
    "          fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))\n",
    "\n",
    "      destination = \"/content/eval-files-2/\"\n",
    "      test_set = glob.glob(destination + \"*[0-9].txt\")\n",
    "\n",
    "      eval_path = \"/content/\"\n",
    "\n",
    "\n",
    "      file_list = [\n",
    "          {\n",
    "          'reference_file': tt,\n",
    "          'estimated_file': tt.replace(\".txt\",\"-se-prediction.txt\")\n",
    "          }\n",
    "          for tt in test_set\n",
    "      ]\n",
    "\n",
    "      data = []\n",
    "\n",
    "      # Get used event labels\n",
    "      all_data = dcase_util.containers.MetaDataContainer()\n",
    "      for file_pair in file_list:\n",
    "          reference_event_list = sed_eval.io.load_event_list(\n",
    "              filename=file_pair['reference_file']\n",
    "          )\n",
    "          estimated_event_list = sed_eval.io.load_event_list(\n",
    "              filename=file_pair['estimated_file']\n",
    "          )\n",
    "\n",
    "          data.append({'reference_event_list': reference_event_list,\n",
    "                      'estimated_event_list': estimated_event_list})\n",
    "\n",
    "          all_data += reference_event_list\n",
    "\n",
    "      event_labels = all_data.unique_event_labels\n",
    "\n",
    "      # Start evaluating\n",
    "\n",
    "      # Create metrics classes, define parameters\n",
    "      segment_based_metrics = sed_eval.sound_event.SegmentBasedMetrics(\n",
    "          event_label_list=event_labels,\n",
    "          time_resolution=1.0\n",
    "      )\n",
    "\n",
    "      event_based_metrics = sed_eval.sound_event.EventBasedMetrics(\n",
    "          event_label_list=event_labels,\n",
    "          t_collar=1.0\n",
    "      )\n",
    "\n",
    "      # Go through files\n",
    "      for file_pair in data:\n",
    "          segment_based_metrics.evaluate(\n",
    "              reference_event_list=file_pair['reference_event_list'],\n",
    "              estimated_event_list=file_pair['estimated_event_list']\n",
    "          )\n",
    "\n",
    "          event_based_metrics.evaluate(\n",
    "              reference_event_list=file_pair['reference_event_list'],\n",
    "              estimated_event_list=file_pair['estimated_event_list']\n",
    "          )\n",
    "\n",
    "      # Get only certain metrics\n",
    "      overall_segment_based_metrics = segment_based_metrics.results_overall_metrics()\n",
    "      curr_f1 = overall_segment_based_metrics['f_measure']['f_measure']\n",
    "      curr_error = overall_segment_based_metrics['error_rate']['error_rate']\n",
    "\n",
    "      wandb.log({\"f_measure\":curr_f1, 'curr_error': curr_error, 'validation_loss':logs['val_loss'], \n",
    "               'training_loss': logs['loss'], 'epoch': epoch})\n",
    "      print(logs)\n",
    "      if curr_f1 > self.best_f1:\n",
    "        self.best_f1 = curr_f1\n",
    "        self.model.save_weights(\"/content/model-best-f1.h5\")\n",
    "        wandb.save('/content/model-best-f1.h5')\n",
    "\n",
    "\n",
    "      if curr_error < self.best_error:\n",
    "        self.best_error = curr_error\n",
    "        self.model.save_weights(\"/content/model-best-error.h5\")\n",
    "        wandb.save('/content/model-best-error.h5')\n",
    "\n",
    "\n",
    "      print(\"F-measure: {:.3f} vs {:.3f}\".format(curr_f1, self.best_f1))\n",
    "      print(\"Error rate: {:.3f} vs {:.3f}\".format(curr_error, self.best_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOHOBlock:\n",
    "  def __init__(self, stride, num_filters, index, input):\n",
    "      X = tf.keras.layers.DepthwiseConv2D(kernel_size=[3,3], strides = stride, depth_multiplier=1, padding='same', use_bias=False,\n",
    "                                      activation=None, name=\"layer\"+ str(index + 2)+\"/depthwise_conv\")(input)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/depthwise_conv/bn\")(X)\n",
    "      X = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/depthwise_conv/relu\")(X)\n",
    "      X = tf.keras.layers.Conv2D(filters =num_filters, kernel_size=[1, 1], strides=1, padding='same', use_bias=False, activation=None,\n",
    "                                name = \"layer\"+ str(index + 2)+\"/pointwise_conv\",\n",
    "                                kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(X)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/pointwise_conv/bn\")(X)\n",
    "      self.output = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/pointwise_conv/relu\")(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujnpubM3Blr-"
   },
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(257, 40), name=\"mel_input\")\n",
    "X = tf.keras.layers.Reshape((257, 40, 1))(input_layer)\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=X,\n",
    "    pooling=None,\n",
    ")\n",
    "at_attn = base_model.output\n",
    "attn_layer = tf.keras.layers.Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(tf.keras.layers.Dropout(0.5)(at_attn))\n",
    "attn_layer = tf.keras.layers.Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = tf.keras.layers.Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = tf.keras.layers.Conv2D(1, \n",
    "                    kernel_size = (1,1), \n",
    "                    padding = 'valid', \n",
    "                    activation = 'sigmoid')(attn_layer)\n",
    "pt_depth = 1280\n",
    "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "up_c2 = tf.keras.layers.Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "up_c2.trainable = False\n",
    "attn_layer = up_c2(attn_layer)\n",
    "\n",
    "mask_features = tf.keras.layers.multiply([attn_layer, at_attn])\n",
    "# gap_features = tf.keras.layers.GlobalAveragePooling2D()(mask_features)\n",
    "# gap_mask = tf.keras.layers.GlobalAveragePooling2D()(attn_layer)\n",
    "# to account for missing values from the attention model\n",
    "gap = tf.keras.layers.Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([mask_features, attn_layer])\n",
    "X = YOHOBlock(stride=1, num_filters=512, index=1, input=gap).output\n",
    "X = YOHOBlock(stride=1, num_filters=256, index=2, input=X).output\n",
    "X = YOHOBlock(stride=1, num_filters=128, index=3, input=X).output\n",
    "_, _, sx, sy = X.shape\n",
    "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\n",
    "pred = tf.keras.layers.Conv1D(18,kernel_size=1, activation=\"sigmoid\")(X)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQsXSvgyBoZM",
    "outputId": "764b2c51-46fe-4739-a119-926055e40dc0"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSJFYjO4BqBQ"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "              loss=square_difference_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FQFJ5yiB1OY",
    "outputId": "8df6b4b6-8f0e-4ef5-b6c0-dd5ca592920f"
   },
   "outputs": [],
   "source": [
    "model.fit(training_generator, validation_data=validation_generator, epochs=300, callbacks=[KerasFinalCallback()], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfWjFws27UM1"
   },
   "outputs": [],
   "source": [
    "run.finish()\n",
    "test_loss = model.evaluate(test_generator)\n",
    "print('test_loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj0i6hmMvhjw"
   },
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6CN3GsuvkTS"
   },
   "outputs": [],
   "source": [
    "def create_mel_spectrograms(audio_file):\n",
    "  win_length = 2.56\n",
    "  hop_size = 1.96\n",
    "\n",
    "  a, win_ranges = construct_examples(audio_file, win_len=win_length,hop_len=hop_size)\n",
    "\n",
    "  mss_in = np.zeros((len(a), 257, 40))\n",
    "\n",
    "  preds = np.zeros((len(a), 9, 18))\n",
    "\n",
    "  for i in range(len(a)):\n",
    "    M = get_log_melspectrogram(a[i])\n",
    "    mss_in[i, :, :] = M.T\n",
    "\n",
    "  return mss_in,win_ranges\n",
    "\n",
    "\n",
    "def run_inference(model, win_ranges, mss_in, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
    "  preds = model.predict(mss_in)\n",
    "  events = []\n",
    "\n",
    "  for i in range(len(preds)):\n",
    "    p = preds[i, :, :]\n",
    "    events_curr = []\n",
    "    win_width = win_length / no_of_div\n",
    "    for predIdx in range(len(p)):\n",
    "      for classIdx in range(0, 6):\n",
    "        if p[predIdx][classIdx*3] >= 0.5:\n",
    "          start = win_width * predIdx + win_width * p[predIdx][classIdx*3+1] + win_ranges[i][0]\n",
    "          end = p[predIdx][classIdx*3+2] * win_width + start\n",
    "          events_curr.append([start, end, rev_class_list[classIdx]])\n",
    "\n",
    "    events += events_curr\n",
    "\n",
    "\n",
    "  class_set = set([c[2] for c in events])\n",
    "  class_wise_events = {}\n",
    "\n",
    "  for c in list(class_set):\n",
    "    class_wise_events[c] = []\n",
    "\n",
    "\n",
    "  for c in events:\n",
    "    class_wise_events[c[2]].append(c)\n",
    "    \n",
    "  \n",
    "  all_events = []\n",
    "\n",
    "  for k in list(class_wise_events.keys()):\n",
    "    curr_events = class_wise_events[k]\n",
    "    count = 0\n",
    "\n",
    "    while count < len(curr_events) - 1:\n",
    "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
    "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
    "        del curr_events[count + 1]\n",
    "      else:\n",
    "        count += 1\n",
    "\n",
    "    all_events += curr_events\n",
    "\n",
    "  for i in range(len(all_events)):\n",
    "    all_events[i][0] = round(all_events[i][0], 3)\n",
    "    all_events[i][1] = round(all_events[i][1], 3)\n",
    "\n",
    "  all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "  return all_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkP-EWfSwsFO",
    "outputId": "14703885-1696-4be1-a677-9168440c5ea7"
   },
   "outputs": [],
   "source": [
    "# Ensure that you have a test-inference.wav file in the correct location (or change the path to the audio file below)\n",
    "def infer_events(model, audio_file):\n",
    "\n",
    "  # create a temp file with single channel of the audio\n",
    "  temp_file = audio_file.replace(\"test\", \"test-mono\")\n",
    "  command = command = \"sox \" + audio_file + \" \" + temp_file + \" channels 1\"\n",
    "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "  output, err = p.communicate()\n",
    "\n",
    "  # make the audio into the melspectrograms\n",
    "  mss_in, win_ranges = create_mel_spectrograms(temp_file)\n",
    "\n",
    "\n",
    "  # run inference to generate the set of events\n",
    "  events = run_inference(model, win_ranges, mss_in)\n",
    "  output_file = \"test-inference.txt\"\n",
    "\n",
    "\n",
    "  with open(output_file, 'w') as fp:\n",
    "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in events))\n",
    "\n",
    "infer_events(model, '/content/test-inference.wav')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2844f9b545184ae4857293bee5a31052": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0766ee9e6024542b4a4fd76130ff0e7",
      "placeholder": "​",
      "style": "IPY_MODEL_b3124f87e101407b8fa4fe77b93ca84a",
      "value": "30.691 MB of 30.746 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "71f6ebf10eac41abb093e05fa07fbfdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0766ee9e6024542b4a4fd76130ff0e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a23928937eaf4a10b731d26b02d3880e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1271aa0be754075b10daa109e3e5605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3124f87e101407b8fa4fe77b93ca84a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1d70626f56c4ef291d1322ee86e1a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1271aa0be754075b10daa109e3e5605",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a23928937eaf4a10b731d26b02d3880e",
      "value": 0.9981968970885496
     }
    },
    "d86553659a904b3aa3f8a16e1bc9309e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2844f9b545184ae4857293bee5a31052",
       "IPY_MODEL_c1d70626f56c4ef291d1322ee86e1a3c"
      ],
      "layout": "IPY_MODEL_71f6ebf10eac41abb093e05fa07fbfdf"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
