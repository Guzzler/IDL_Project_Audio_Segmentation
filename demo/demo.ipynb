{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB1i1GWBHMtA"
   },
   "source": [
    "# Initial Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EH91UBWmXA1"
   },
   "outputs": [],
   "source": [
    "# Ensure that you are within the demo folder with all the files within it and are running this in a google colab environment. If not you might need to change some of the depedencies mentioned and run this command below\n",
    "# !pip install -r demo-requirements.txt\n",
    "\n",
    "# Check libcudnn8 version\n",
    "# !apt-cache policy libcudnn8\n",
    "\n",
    "# # Install latest version\n",
    "# !apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6\n",
    "\n",
    "# # Export env variables\n",
    "# !export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}\n",
    "# !export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\n",
    "# !export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH\n",
    "# !export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64\n",
    "\n",
    "# # Install tensorflow\n",
    "# !pip install tflite-model-maker==0.4.0\n",
    "# !pip uninstall -y tensorflow && pip install -q tensorflow==2.9.1\n",
    "# !pip install pycocotools==2.0.4\n",
    "# !pip install opencv-python-headless==4.6.0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJjOWmynluge",
    "outputId": "e927b45c-2eec-4928-ac5a-e724ca284267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "sox is already the newest version (14.4.2+git20190427-2+deb11u2build0.20.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
      "fatal: destination path 'SpecAugment' already exists and is not an empty directory.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for SpecAugment (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "from subprocess import Popen, PIPE\n",
    "from keras import regularizers\n",
    "from os.path import dirname\n",
    "import os\n",
    "import soundfile as sf\n",
    "!sudo apt-get install sox\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "!git clone https://github.com/DemisEom/SpecAugment.git\n",
    "!pip install /content/SpecAugment/ --quiet\n",
    "!pip install tensorflow-addons --quiet\n",
    "!pip install sed_eval --quiet\n",
    "import keras\n",
    "from SpecAugment import spec_augment_tensorflow\n",
    "import sed_eval\n",
    "import dcase_util\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oplts-YdlJeV"
   },
   "source": [
    "# Download preset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "NRQ9DlXiWdYi",
    "outputId": "a93e7931-2843-4b91-aa6d-db732aa519ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.9/dist-packages (4.6.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown) (4.11.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.10.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1cnDPfL3udjHcT516qX9mcXigvE_WYha3\n",
      "To: /content/best-model.h5\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 144MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/best-model.h5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "output1 = \"best-model.h5\"\n",
    "gdown.download(id='1cnDPfL3udjHcT516qX9mcXigvE_WYha3', output=output1, quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkRc04QOHwA3"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v36BZwiSDZg1"
   },
   "outputs": [],
   "source": [
    "def convert_annotations_to_events(filename): #read_annotations\n",
    "    events = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            row.append(row[0])\n",
    "            row.pop(0)\n",
    "            row[1] = str((float(row[1])/1000))\n",
    "            row[0] = str((float(row[0])/1000))\n",
    "            events.append(row)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUElUf__xD3M"
   },
   "outputs": [],
   "source": [
    "def construct_examples(audio_path, win_len = 2.56, hop_len = 1.0, sr = 44100.0):\n",
    "  # here win_len is the window_length and hop_len is the hop_length between the examples.\n",
    "  # sr is the sampling rate\n",
    "\n",
    "  window_length_t = win_len\n",
    "  hop_length_t = hop_len\n",
    "\n",
    "  window_length = int(sr*window_length_t)\n",
    "  hop_length = int(sr*hop_length_t)\n",
    "\n",
    "  audio, sr = sf.read(audio_path)\n",
    "\n",
    "  # handle padding\n",
    "  if audio.shape[0] < window_length:\n",
    "    audio_padded = np.zeros((window_length, ))\n",
    "    audio_padded[0:audio.shape[0]] = audio \n",
    "\n",
    "  else:\n",
    "    no_of_hops = math.ceil((audio.shape[0] - window_length) / hop_length)\n",
    "    audio_padded = np.zeros((int(window_length + hop_length*no_of_hops), ))\n",
    "    audio_padded[0:audio.shape[0]] = audio  \n",
    "\n",
    "  audio_example = [audio_padded[i - window_length : i] for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "  win_ranges = [((i - window_length)/sr, i/sr) for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "\n",
    "  return audio_example, win_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eh8YoqFVNhVf"
   },
   "outputs": [],
   "source": [
    "CLASS_ENCODING = {\"car\": 0, \"aircraft\": 1, \"crowds\":2, \"footsteps\":3, \"clocks\":4, \"rainforest\": 5}\n",
    "rev_class_list = list(CLASS_ENCODING.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Tz6t6EslAhsJ"
   },
   "outputs": [],
   "source": [
    "def get_log_melspectrogram(audio, sr = 44100, hop_length = 441, win_length = 1764, n_fft = 2048, n_mels = 40, fmin = 0, fmax = 22050):\n",
    "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
    "    audio_2 = librosa.util.normalize(audio)\n",
    "    bands = librosa.feature.melspectrogram(\n",
    "        y=audio_2, sr=sr, hop_length=hop_length, win_length = win_length, n_fft=n_fft, n_mels=n_mels)\n",
    "    return librosa.core.power_to_db(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TR0orxHhmXA6"
   },
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bwK_9wF8rjB_"
   },
   "outputs": [],
   "source": [
    "class YOHOBlock:\n",
    "  def __init__(self, stride, num_filters, index, input):\n",
    "      X = tf.keras.layers.DepthwiseConv2D(kernel_size=[3,3], strides = stride, depth_multiplier=1, padding='same', use_bias=False,\n",
    "                                      activation=None, name=\"layer\"+ str(index + 2)+\"/depthwise_conv\")(input)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/depthwise_conv/bn\")(X)\n",
    "      X = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/depthwise_conv/relu\")(X)\n",
    "      X = tf.keras.layers.Conv2D(filters =num_filters, kernel_size=[1, 1], strides=1, padding='same', use_bias=False, activation=None,\n",
    "                                name = \"layer\"+ str(index + 2)+\"/pointwise_conv\",\n",
    "                                kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(X)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/pointwise_conv/bn\")(X)\n",
    "      self.output = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/pointwise_conv/relu\")(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNQ6IkGsri1G"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ujnpubM3Blr-"
   },
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape=(257, 40), name=\"mel_input\")\n",
    "X = tf.keras.layers.Reshape((257, 40, 1))(input_layer)\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=X,\n",
    "    pooling=None,\n",
    ")\n",
    "at_attn = base_model.output\n",
    "attn_layer = tf.keras.layers.Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(tf.keras.layers.Dropout(0.5)(at_attn))\n",
    "attn_layer = tf.keras.layers.Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = tf.keras.layers.Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = tf.keras.layers.Conv2D(1, \n",
    "                    kernel_size = (1,1), \n",
    "                    padding = 'valid', \n",
    "                    activation = 'sigmoid')(attn_layer)\n",
    "pt_depth = 1280\n",
    "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "up_c2 = tf.keras.layers.Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "up_c2.trainable = False\n",
    "attn_layer = up_c2(attn_layer)\n",
    "\n",
    "mask_features = tf.keras.layers.multiply([attn_layer, at_attn])\n",
    "\n",
    "gap = tf.keras.layers.Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([mask_features, attn_layer])\n",
    "X = YOHOBlock(stride=1, num_filters=512, index=1, input=gap).output\n",
    "X = YOHOBlock(stride=1, num_filters=256, index=2, input=X).output\n",
    "X = YOHOBlock(stride=1, num_filters=128, index=3, input=X).output\n",
    "_, _, sx, sy = X.shape\n",
    "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\n",
    "pred = tf.keras.layers.Conv1D(18,kernel_size=1, activation=\"sigmoid\")(X)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auQ01-lYmXA7"
   },
   "source": [
    "# Load the preset model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PPn9rjvjmXA7"
   },
   "outputs": [],
   "source": [
    "model.load_weights('best-model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj0i6hmMvhjw"
   },
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "j6CN3GsuvkTS"
   },
   "outputs": [],
   "source": [
    "def create_mel_spectrograms(audio_file):\n",
    "  win_length = 2.56\n",
    "  hop_size = 1.96\n",
    "\n",
    "  a, win_ranges = construct_examples(audio_file, win_len=win_length,hop_len=hop_size)\n",
    "\n",
    "  mss_in = np.zeros((len(a), 257, 40))\n",
    "\n",
    "  preds = np.zeros((len(a), 9, 18))\n",
    "\n",
    "  for i in range(len(a)):\n",
    "    M = get_log_melspectrogram(a[i])\n",
    "    mss_in[i, :, :] = M.T\n",
    "\n",
    "  return mss_in,win_ranges\n",
    "\n",
    "\n",
    "def run_inference(model, win_ranges, mss_in, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
    "  preds = model.predict(mss_in)\n",
    "  events = []\n",
    "\n",
    "  for i in range(len(preds)):\n",
    "    p = preds[i, :, :]\n",
    "    events_curr = []\n",
    "    win_width = win_length / no_of_div\n",
    "    for predIdx in range(len(p)):\n",
    "      for classIdx in range(0, 6):\n",
    "        if p[predIdx][classIdx*3] >= 0.5:\n",
    "          start = win_width * predIdx + win_width * p[predIdx][classIdx*3+1] + win_ranges[i][0]\n",
    "          end = p[predIdx][classIdx*3+2] * win_width + start\n",
    "          events_curr.append([start, end, rev_class_list[classIdx]])\n",
    "\n",
    "    events += events_curr\n",
    "\n",
    "\n",
    "  class_set = set([c[2] for c in events])\n",
    "  class_wise_events = {}\n",
    "\n",
    "  for c in list(class_set):\n",
    "    class_wise_events[c] = []\n",
    "\n",
    "\n",
    "  for c in events:\n",
    "    class_wise_events[c[2]].append(c)\n",
    "    \n",
    "  \n",
    "  all_events = []\n",
    "\n",
    "  for k in list(class_wise_events.keys()):\n",
    "    curr_events = class_wise_events[k]\n",
    "    count = 0\n",
    "\n",
    "    while count < len(curr_events) - 1:\n",
    "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
    "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
    "        del curr_events[count + 1]\n",
    "      else:\n",
    "        count += 1\n",
    "\n",
    "    all_events += curr_events\n",
    "\n",
    "  for i in range(len(all_events)):\n",
    "    all_events[i][0] = round(all_events[i][0], 3)\n",
    "    all_events[i][1] = round(all_events[i][1], 3)\n",
    "\n",
    "  all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "  return all_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkP-EWfSwsFO"
   },
   "outputs": [],
   "source": [
    "# Ensure that you have a test-inference.wav file in the correct location (or change the path to the audio file below)\n",
    "def infer_events(model, audio_file):\n",
    "\n",
    "  # create a temp file with single channel of the audio\n",
    "  temp_file = audio_file.replace(\"test\", \"test-mono\")\n",
    "  command = command = \"sox \" + audio_file + \" \" + temp_file + \" channels 1\"\n",
    "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "  output, err = p.communicate()\n",
    "\n",
    "  # make the audio into the melspectrograms\n",
    "  mss_in, win_ranges = create_mel_spectrograms(temp_file)\n",
    "\n",
    "\n",
    "  # run inference to generate the set of events\n",
    "  events = run_inference(model, win_ranges, mss_in)\n",
    "  output_file = \"test-inference.txt\"\n",
    "\n",
    "  print('outputted segmentation events')\n",
    "  print(events)\n",
    "  with open(output_file, 'w') as fp:\n",
    "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in events))\n",
    "\n",
    "infer_events(model, 'sample-mixed-audio.wav')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
