{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB1i1GWBHMtA"
   },
   "source": [
    "# Initial Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that you are within the demo folder with all the files within it and are running this in a google colab environment. If not you might need to change some of the depedencies mentioned\n",
    "# !pip install -r demo-requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJjOWmynluge"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import glob\n",
    "import csv\n",
    "import random\n",
    "from subprocess import Popen, PIPE\n",
    "from keras import regularizers\n",
    "from os.path import dirname\n",
    "import os\n",
    "import soundfile as sf\n",
    "!sudo apt-get install sox\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import shutil\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "!git clone https://github.com/DemisEom/SpecAugment.git\n",
    "!pip install /content/SpecAugment/ --quiet\n",
    "!pip install tensorflow-addons --quiet\n",
    "!pip install sed_eval --quiet\n",
    "import keras\n",
    "from SpecAugment import spec_augment_tensorflow\n",
    "import sed_eval\n",
    "import dcase_util\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oplts-YdlJeV"
   },
   "source": [
    "# Download preset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "NRQ9DlXiWdYi",
    "outputId": "3d0e523d-0bd9-485c-ec86-1f8b2f3a209c"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "\n",
    "output1 = \"best_model.h5\"\n",
    "gdown.download(id='1cnDPfL3udjHcT516qX9mcXigvE_WYha3', output=output1, quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkRc04QOHwA3"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v36BZwiSDZg1"
   },
   "outputs": [],
   "source": [
    "def convert_annotations_to_events(filename): #read_annotations\n",
    "    events = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            row.append(row[0])\n",
    "            row.pop(0)\n",
    "            row[1] = str((float(row[1])/1000))\n",
    "            row[0] = str((float(row[0])/1000))\n",
    "            events.append(row)\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUElUf__xD3M"
   },
   "outputs": [],
   "source": [
    "def construct_examples(audio_path, win_len = 2.56, hop_len = 1.0, sr = 44100.0):\n",
    "  # here win_len is the window_length and hop_len is the hop_length between the examples.\n",
    "  # sr is the sampling rate\n",
    "\n",
    "  window_length_t = win_len\n",
    "  hop_length_t = hop_len\n",
    "\n",
    "  window_length = int(sr*window_length_t)\n",
    "  hop_length = int(sr*hop_length_t)\n",
    "\n",
    "  audio, sr = sf.read(audio_path)\n",
    "\n",
    "  # handle padding\n",
    "  if audio.shape[0] < window_length:\n",
    "    audio_padded = np.zeros((window_length, ))\n",
    "    audio_padded[0:audio.shape[0]] = audio \n",
    "\n",
    "  else:\n",
    "    no_of_hops = math.ceil((audio.shape[0] - window_length) / hop_length)\n",
    "    audio_padded = np.zeros((int(window_length + hop_length*no_of_hops), ))\n",
    "    audio_padded[0:audio.shape[0]] = audio  \n",
    "\n",
    "  audio_example = [audio_padded[i - window_length : i] for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "  win_ranges = [((i - window_length)/sr, i/sr) for i in range(window_length, audio_padded.shape[0]+1, hop_length)]\n",
    "\n",
    "  return audio_example, win_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh8YoqFVNhVf"
   },
   "outputs": [],
   "source": [
    "CLASS_ENCODING = {\"car\": 0, \"aircraft\": 1, \"crowds\":2, \"footsteps\":3, \"clocks\":4, \"rainforest\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tz6t6EslAhsJ"
   },
   "outputs": [],
   "source": [
    "def get_log_melspectrogram(audio, sr = 44100, hop_length = 441, win_length = 1764, n_fft = 2048, n_mels = 128, fmin = 0, fmax = 22050):\n",
    "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
    "    audio_2 = librosa.util.normalize(audio)\n",
    "    bands = librosa.feature.melspectrogram(\n",
    "        y=audio_2, sr=sr, hop_length=hop_length, win_length = win_length, n_fft=n_fft, n_mels=n_mels)\n",
    "    return librosa.core.power_to_db(bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujnpubM3Blr-"
   },
   "outputs": [],
   "source": [
    "class YOHOBlock:\n",
    "  def __init__(self, stride, num_filters, index, input):\n",
    "      X = tf.keras.layers.DepthwiseConv2D(kernel_size=[3,3], strides = stride, depth_multiplier=1, padding='same', use_bias=False,\n",
    "                                      activation=None, name=\"layer\"+ str(index + 2)+\"/depthwise_conv\")(input)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/depthwise_conv/bn\")(X)\n",
    "      X = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/depthwise_conv/relu\")(X)\n",
    "      X = tf.keras.layers.Conv2D(filters =num_filters, kernel_size=[1, 1], strides=1, padding='same', use_bias=False, activation=None,\n",
    "                                name = \"layer\"+ str(index + 2)+\"/pointwise_conv\",\n",
    "                                kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(X)\n",
    "      X = tf.keras.layers.BatchNormalization(center=True, scale=False, epsilon=1e-4, name = \"layer\"+ str(index + 2)+\"/pointwise_conv/bn\")(X)\n",
    "      self.output = tf.keras.layers.ReLU(name=\"layer\"+ str(index + 2)+\"/pointwise_conv/relu\")(X)\n",
    "\n",
    "\n",
    "input_layer = tf.keras.Input(shape=(257, 40), name=\"mel_input\")\n",
    "X = tf.keras.layers.Reshape((257, 40, 1))(input_layer)\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=X,\n",
    "    pooling=None,\n",
    ")\n",
    "X = base_model.output\n",
    "X = YOHOBlock(stride=1, num_filters=512, index=1, input=X).output\n",
    "X = YOHOBlock(stride=1, num_filters=256, index=2, input=X).output\n",
    "X = YOHOBlock(stride=1, num_filters=128, index=3, input=X).output\n",
    "_, _, sx, sy = X.shape\n",
    "X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)\n",
    "pred = tf.keras.layers.Conv1D(18,kernel_size=1, activation=\"sigmoid\")(X)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preset model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/content/best-model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj0i6hmMvhjw"
   },
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6CN3GsuvkTS"
   },
   "outputs": [],
   "source": [
    "def create_mel_spectrograms(audio_file):\n",
    "  win_length = 2.56\n",
    "  hop_size = 1.96\n",
    "\n",
    "  a, win_ranges = construct_examples(audio_file, win_len=win_length,hop_len=hop_size)\n",
    "\n",
    "  mss_in = np.zeros((len(a), 257, 128))\n",
    "\n",
    "  preds = np.zeros((len(a), 9, 18))\n",
    "\n",
    "  for i in range(len(a)):\n",
    "    M = get_log_melspectrogram(a[i])\n",
    "    mss_in[i, :, :] = M.T\n",
    "\n",
    "  return mss_in,win_ranges\n",
    "\n",
    "\n",
    "def run_inference(model, win_ranges, mss_in, no_of_div = 9, hop_size = 1.96, discard = 0.3, win_length = 2.56, max_event_silence = 0.3, sampling_rate = 44100):\n",
    "  preds = model.predict(mss_in)\n",
    "  events = []\n",
    "\n",
    "  for i in range(len(preds)):\n",
    "    p = preds[i, :, :]\n",
    "    events_curr = []\n",
    "    win_width = win_length / no_of_div\n",
    "    for predIdx in range(len(p)):\n",
    "      for classIdx in range(0, 6):\n",
    "        if p[predIdx][classIdx*3] >= 0.5:\n",
    "          start = win_width * predIdx + win_width * p[predIdx][classIdx*3+1] + win_ranges[i][0]\n",
    "          end = p[predIdx][classIdx*3+2] * win_width + start\n",
    "          events_curr.append([start, end, rev_class_list[classIdx]])\n",
    "\n",
    "    events += events_curr\n",
    "\n",
    "\n",
    "  class_set = set([c[2] for c in events])\n",
    "  class_wise_events = {}\n",
    "\n",
    "  for c in list(class_set):\n",
    "    class_wise_events[c] = []\n",
    "\n",
    "\n",
    "  for c in events:\n",
    "    class_wise_events[c[2]].append(c)\n",
    "    \n",
    "  \n",
    "  all_events = []\n",
    "\n",
    "  for k in list(class_wise_events.keys()):\n",
    "    curr_events = class_wise_events[k]\n",
    "    count = 0\n",
    "\n",
    "    while count < len(curr_events) - 1:\n",
    "      if (curr_events[count][1] >= curr_events[count + 1][0]) or (curr_events[count + 1][0] - curr_events[count][1] <= max_event_silence):\n",
    "        curr_events[count][1] = max(curr_events[count + 1][1], curr_events[count][1])\n",
    "        del curr_events[count + 1]\n",
    "      else:\n",
    "        count += 1\n",
    "\n",
    "    all_events += curr_events\n",
    "\n",
    "  for i in range(len(all_events)):\n",
    "    all_events[i][0] = round(all_events[i][0], 3)\n",
    "    all_events[i][1] = round(all_events[i][1], 3)\n",
    "\n",
    "  all_events.sort(key=lambda x: x[0])\n",
    "\n",
    "  return all_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkP-EWfSwsFO",
    "outputId": "14703885-1696-4be1-a677-9168440c5ea7"
   },
   "outputs": [],
   "source": [
    "# Ensure that you have a test-inference.wav file in the correct location (or change the path to the audio file below)\n",
    "def infer_events(model, audio_file):\n",
    "\n",
    "  # create a temp file with single channel of the audio\n",
    "  temp_file = audio_file.replace(\"test\", \"test-mono\")\n",
    "  command = command = \"sox \" + audio_file + \" \" + temp_file + \" channels 1\"\n",
    "  p = Popen(command, stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True)\n",
    "  output, err = p.communicate()\n",
    "\n",
    "  # make the audio into the melspectrograms\n",
    "  mss_in, win_ranges = create_mel_spectrograms(temp_file)\n",
    "\n",
    "\n",
    "  # run inference to generate the set of events\n",
    "  events = run_inference(model, win_ranges, mss_in)\n",
    "  output_file = \"test-inference.txt\"\n",
    "\n",
    "  print('outputted segmentation events')\n",
    "  print(events)\n",
    "  with open(output_file, 'w') as fp:\n",
    "    fp.write('\\n'.join('{},{},{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in events))\n",
    "\n",
    "infer_events(model, 'sample-mixed-audio.wav')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2844f9b545184ae4857293bee5a31052": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0766ee9e6024542b4a4fd76130ff0e7",
      "placeholder": "​",
      "style": "IPY_MODEL_b3124f87e101407b8fa4fe77b93ca84a",
      "value": "30.691 MB of 30.746 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "71f6ebf10eac41abb093e05fa07fbfdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0766ee9e6024542b4a4fd76130ff0e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a23928937eaf4a10b731d26b02d3880e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1271aa0be754075b10daa109e3e5605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3124f87e101407b8fa4fe77b93ca84a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1d70626f56c4ef291d1322ee86e1a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1271aa0be754075b10daa109e3e5605",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a23928937eaf4a10b731d26b02d3880e",
      "value": 0.9981968970885496
     }
    },
    "d86553659a904b3aa3f8a16e1bc9309e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2844f9b545184ae4857293bee5a31052",
       "IPY_MODEL_c1d70626f56c4ef291d1322ee86e1a3c"
      ],
      "layout": "IPY_MODEL_71f6ebf10eac41abb093e05fa07fbfdf"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
